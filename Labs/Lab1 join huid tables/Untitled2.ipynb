{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6206ed3a-3184-4def-83ee-511aff084fe6",
   "metadata": {},
   "source": [
    "# Create Spark Session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30769a19-ee4d-4d45-86c0-36c6687d5b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.sql.hive.convertMetastoreParquet': 'false', 'spark.sql.catalog.spark_catalog': 'org.apache.spark.sql.hudi.catalog.HoodieCatalog', 'spark.sql.legacy.pathOptionBehavior.enabled': 'true', 'spark.sql.extensions': 'org.apache.spark.sql.hudi.HoodieSparkSessionExtension'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "\"conf\": {\n",
    "\"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n",
    "\"spark.sql.hive.convertMetastoreParquet\": \"false\",\n",
    "\"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\",\n",
    "\"spark.sql.legacy.pathOptionBehavior.enabled\": \"true\",\n",
    "\"spark.sql.extensions\": \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\"\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ff5f14c-febf-456d-b70a-1d85236eb1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: Faker in /home/glue_user/.local/lib/python3.10/site-packages (21.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /home/glue_user/.local/lib/python3.10/site-packages (from Faker) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/glue_user/.local/lib/python3.10/site-packages (from python-dateutil>=2.4->Faker) (1.16.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a9b9297-9928-4799-8c10-9b067d932e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'3.3.0-amzn-1'"
     ]
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77239001-9099-4621-aac1-910437085976",
   "metadata": {},
   "source": [
    "# Define helper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88d00e92-2aa4-40d0-a9b7-552789bb0a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "%%configure -f\n",
    "{\n",
    "\"conf\": {\n",
    "\"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\",\n",
    "\"spark.sql.hive.convertMetastoreParquet\": \"false\",\n",
    "\"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\",\n",
    "\"spark.sql.legacy.pathOptionBehavior.enabled\": \"true\",\n",
    "\"spark.sql.extensions\": \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\"\n",
    "}\n",
    "}\n",
    "\n",
    "\n",
    "%pip install Faker\n",
    "\n",
    "\n",
    "permanently delete\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "try:\n",
    "    import sys, random, uuid\n",
    "    from pyspark.context import SparkContext\n",
    "    from pyspark.sql.session import SparkSession\n",
    "    from awsglue.context import GlueContext\n",
    "    from awsglue.job import Job\n",
    "    from awsglue.dynamicframe import DynamicFrame\n",
    "    from pyspark.sql.functions import col, to_timestamp, monotonically_increasing_id, to_date, when\n",
    "    from pyspark.sql.functions import *\n",
    "    from awsglue.utils import getResolvedOptions\n",
    "    from pyspark.sql.types import *\n",
    "    from datetime import datetime, date\n",
    "    import boto3, pandas\n",
    "    from functools import reduce\n",
    "    from pyspark.sql import Row\n",
    "    from faker import Faker\n",
    "except Exception as e:\n",
    "    print(\"Modules are missing : {} \".format(e))\n",
    "\n",
    "job_start_ts = datetime.now()\n",
    "ts_format = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "spark = (SparkSession.builder.config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer') \\\n",
    "         .config('spark.sql.hive.convertMetastoreParquet', 'false') \\\n",
    "         .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.hudi.catalog.HoodieCatalog') \\\n",
    "         .config('spark.sql.extensions', 'org.apache.spark.sql.hudi.HoodieSparkSessionExtension') \\\n",
    "         .config('spark.sql.legacy.pathOptionBehavior.enabled', 'true').getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext\n",
    "glueContext = GlueContext(sc)\n",
    "job = Job(glueContext)\n",
    "logger = glueContext.get_logger()\n",
    "\n",
    "global faker\n",
    "faker = Faker()\n",
    "\n",
    "\n",
    "def get_customer_data(total_customers=2):\n",
    "    customers_array = []\n",
    "    for i in range(0, total_customers):\n",
    "        customer_data = {\n",
    "            \"customer_id\": str(uuid.uuid4()),\n",
    "            \"name\": faker.name(),\n",
    "            \"state\": faker.state(),\n",
    "            \"city\": faker.city(),\n",
    "            \"email\": faker.email(),\n",
    "            \"created_at\": datetime.now().isoformat().__str__(),\n",
    "            \"address\": faker.address(),\n",
    "\n",
    "        }\n",
    "        customers_array.append(customer_data)\n",
    "    return customers_array\n",
    "\n",
    "\n",
    "def get_orders_data(customer_ids, order_data_sample_size=3):\n",
    "    orders_array = []\n",
    "    for i in range(0, order_data_sample_size):\n",
    "        try:\n",
    "            order_id = uuid.uuid4().__str__()\n",
    "            customer_id = random.choice(customer_ids)\n",
    "            order_data = {\n",
    "                \"order_id\": order_id,\n",
    "                \"name\": faker.text(max_nb_chars=20),\n",
    "                \"order_value\": random.randint(10, 1000).__str__(),\n",
    "                \"priority\": random.choice([\"LOW\", \"MEDIUM\", \"HIGH\"]),\n",
    "                \"order_date\": faker.date_between(start_date='-30d', end_date='today').strftime('%Y-%m-%d'),\n",
    "                \"customer_id\": customer_id,\n",
    "\n",
    "            }\n",
    "            orders_array.append(order_data)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return orders_array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def upsert_hudi_table(glue_database, table_name, record_id, precomb_key, table_type, spark_df, partition_fields,\n",
    "                      enable_partition, enable_cleaner, enable_hive_sync, enable_clustering,\n",
    "                      enable_meta_data_indexing,\n",
    "                      use_sql_transformer, sql_transformer_query,\n",
    "                      target_path, index_type, method='upsert', clustering_column='default'):\n",
    "    \"\"\"\n",
    "    Upserts a dataframe into a Hudi table.\n",
    "\n",
    "    Args:\n",
    "        glue_database (str): The name of the glue database.\n",
    "        table_name (str): The name of the Hudi table.\n",
    "        record_id (str): The name of the field in the dataframe that will be used as the record key.\n",
    "        precomb_key (str): The name of the field in the dataframe that will be used for pre-combine.\n",
    "        table_type (str): The Hudi table type (e.g., COPY_ON_WRITE, MERGE_ON_READ).\n",
    "        spark_df (pyspark.sql.DataFrame): The dataframe to upsert.\n",
    "        partition_fields this is used to parrtition data\n",
    "        enable_partition (bool): Whether or not to enable partitioning.\n",
    "        enable_cleaner (bool): Whether or not to enable data cleaning.\n",
    "        enable_hive_sync (bool): Whether or not to enable syncing with Hive.\n",
    "        use_sql_transformer (bool): Whether or not to use SQL to transform the dataframe before upserting.\n",
    "        sql_transformer_query (str): The SQL query to use for data transformation.\n",
    "        target_path (str): The path to the target Hudi table.\n",
    "        method (str): The Hudi write method to use (default is 'upsert').\n",
    "        index_type : BLOOM or GLOBAL_BLOOM\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # These are the basic settings for the Hoodie table\n",
    "    hudi_final_settings = {\n",
    "        \"hoodie.table.name\": table_name,\n",
    "        \"hoodie.datasource.write.table.type\": table_type,\n",
    "        \"hoodie.datasource.write.operation\": method,\n",
    "        \"hoodie.datasource.write.recordkey.field\": record_id,\n",
    "        \"hoodie.datasource.write.precombine.field\": precomb_key,\n",
    "    }\n",
    "\n",
    "    # These settings enable syncing with Hive\n",
    "    hudi_hive_sync_settings = {\n",
    "        \"hoodie.parquet.compression.codec\": \"gzip\",\n",
    "        \"hoodie.datasource.hive_sync.enable\": \"true\",\n",
    "        \"hoodie.datasource.hive_sync.database\": glue_database,\n",
    "        \"hoodie.datasource.hive_sync.table\": table_name,\n",
    "        \"hoodie.datasource.hive_sync.partition_extractor_class\": \"org.apache.hudi.hive.MultiPartKeysValueExtractor\",\n",
    "        \"hoodie.datasource.hive_sync.use_jdbc\": \"false\",\n",
    "        \"hoodie.datasource.hive_sync.mode\": \"hms\",\n",
    "    }\n",
    "\n",
    "    # These settings enable automatic cleaning of old data\n",
    "    hudi_cleaner_options = {\n",
    "        \"hoodie.clean.automatic\": \"true\",\n",
    "        \"hoodie.clean.async\": \"true\",\n",
    "        \"hoodie.cleaner.policy\": 'KEEP_LATEST_FILE_VERSIONS',\n",
    "        \"hoodie.cleaner.fileversions.retained\": \"3\",\n",
    "        \"hoodie-conf hoodie.cleaner.parallelism\": '200',\n",
    "        'hoodie.cleaner.commits.retained': 5\n",
    "    }\n",
    "\n",
    "    # These settings enable partitioning of the data\n",
    "    partition_settings = {\n",
    "        \"hoodie.datasource.write.partitionpath.field\": partition_fields,\n",
    "        \"hoodie.datasource.hive_sync.partition_fields\": partition_fields,\n",
    "        \"hoodie.datasource.write.hive_style_partitioning\": \"true\",\n",
    "    }\n",
    "\n",
    "    hudi_clustering = {\n",
    "        \"hoodie.clustering.execution.strategy.class\": \"org.apache.hudi.client.clustering.run.strategy.SparkSortAndSizeExecutionStrategy\",\n",
    "        \"hoodie.clustering.inline\": \"true\",\n",
    "        \"hoodie.clustering.plan.strategy.sort.columns\": clustering_column,\n",
    "        \"hoodie.clustering.plan.strategy.target.file.max.bytes\": \"1073741824\",\n",
    "        \"hoodie.clustering.plan.strategy.small.file.limit\": \"629145600\"\n",
    "    }\n",
    "\n",
    "    # Define a dictionary with the index settings for Hudi\n",
    "    hudi_index_settings = {\n",
    "        \"hoodie.index.type\": index_type,  # Specify the index type for Hudi\n",
    "    }\n",
    "\n",
    "    # Define a dictionary with the Fiel Size\n",
    "    hudi_file_size = {\n",
    "        \"hoodie.parquet.max.file.size\": 512 * 1024 * 1024,  # 512MB\n",
    "        \"hoodie.parquet.small.file.limit\": 104857600,  # 100MB\n",
    "    }\n",
    "\n",
    "    hudi_meta_data_indexing = {\n",
    "        \"hoodie.metadata.enable\": \"true\",\n",
    "        \"hoodie.metadata.index.async\": \"true\",\n",
    "        \"hoodie.metadata.index.column.stats.enable\": \"true\",\n",
    "        \"hoodie.metadata.index.check.timeout.seconds\": \"60\",\n",
    "        \"hoodie.write.concurrency.mode\": \"optimistic_concurrency_control\",\n",
    "        \"hoodie.write.lock.provider\": \"org.apache.hudi.client.transaction.lock.InProcessLockProvider\"\n",
    "    }\n",
    "\n",
    "    if enable_meta_data_indexing == True or enable_meta_data_indexing == \"True\" or enable_meta_data_indexing == \"true\":\n",
    "        for key, value in hudi_meta_data_indexing.items():\n",
    "            hudi_final_settings[key] = value  # Add the key-value pair to the final settings dictionary\n",
    "\n",
    "    if enable_clustering == True or enable_clustering == \"True\" or enable_clustering == \"true\":\n",
    "        for key, value in hudi_clustering.items():\n",
    "            hudi_final_settings[key] = value  # Add the key-value pair to the final settings dictionary\n",
    "\n",
    "    # Add the Hudi index settings to the final settings dictionary\n",
    "    for key, value in hudi_index_settings.items():\n",
    "        hudi_final_settings[key] = value  # Add the key-value pair to the final settings dictionary\n",
    "\n",
    "    for key, value in hudi_file_size.items():\n",
    "        hudi_final_settings[key] = value  # Add the key-value pair to the final settings dictionary\n",
    "\n",
    "    # If partitioning is enabled, add the partition settings to the final settings\n",
    "    if enable_partition == \"True\" or enable_partition == \"true\" or enable_partition == True:\n",
    "        for key, value in partition_settings.items(): hudi_final_settings[key] = value\n",
    "\n",
    "    # If data cleaning is enabled, add the cleaner options to the final settings\n",
    "    if enable_cleaner == \"True\" or enable_cleaner == \"true\" or enable_cleaner == True:\n",
    "        for key, value in hudi_cleaner_options.items(): hudi_final_settings[key] = value\n",
    "\n",
    "    # If Hive syncing is enabled, add the Hive sync settings to the final settings\n",
    "    if enable_hive_sync == \"True\" or enable_hive_sync == \"true\" or enable_hive_sync == True:\n",
    "        for key, value in hudi_hive_sync_settings.items(): hudi_final_settings[key] = value\n",
    "\n",
    "    # If there is data to write, apply any SQL transformations and write to the target path\n",
    "    if spark_df.count() > 0:\n",
    "        if use_sql_transformer == \"True\" or use_sql_transformer == \"true\" or use_sql_transformer == True:\n",
    "            spark_df.createOrReplaceTempView(\"temp\")\n",
    "            spark_df = spark.sql(sql_transformer_query)\n",
    "\n",
    "        spark_df.write.format(\"hudi\"). \\\n",
    "            options(**hudi_final_settings). \\\n",
    "            mode(\"append\"). \\\n",
    "            save(target_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75264f1-2103-4077-afcc-019b2a8c85cb",
   "metadata": {},
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40b26a5b-2b23-4c93-a850-3234f40f81dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------+-------------+--------------------+--------------------+--------------------+\n",
      "|         customer_id|         name|   state|         city|               email|          created_at|             address|\n",
      "+--------------------+-------------+--------+-------------+--------------------+--------------------+--------------------+\n",
      "|3e908a54-2ea6-46c...|Steven Brewer|Michigan|New Wendyberg|calvinhoover@exam...|2023-12-25T15:14:...|21343 Smith Ways\\...|\n",
      "+--------------------+-------------+--------+-------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "+--------------------+--------------------+-----------+--------+----------+--------------------+\n",
      "|            order_id|                name|order_value|priority|order_date|         customer_id|\n",
      "+--------------------+--------------------+-----------+--------+----------+--------------------+\n",
      "|327bae66-8721-45d...|   Address girl new.|        863|  MEDIUM|2023-12-23|3e908a54-2ea6-46c...|\n",
      "|ff63fded-243c-495...|List example future.|        739|     LOW|2023-12-02|3e908a54-2ea6-46c...|\n",
      "+--------------------+--------------------+-----------+--------+----------+--------------------+"
     ]
    }
   ],
   "source": [
    "# ---------------------------- CUSTOMERS ------------------------------------------------\n",
    "global total_customers, order_data_sample_size\n",
    "\n",
    "total_customers = 1\n",
    "order_data_sample_size = 2\n",
    "\n",
    "customer_data = get_customer_data(total_customers=total_customers)\n",
    "\n",
    "order_data = get_orders_data(\n",
    "    order_data_sample_size=order_data_sample_size,\n",
    "    customer_ids=[i.get(\"customer_id\") for i in customer_data]\n",
    ")\n",
    "\n",
    "spark_df_customers = spark.createDataFrame(data=[tuple(i.values()) for i in customer_data],\n",
    "                                           schema=list(customer_data[0].keys()))\n",
    "spark_df_customers.show()\n",
    "spark_df_orders = spark.createDataFrame(data=[tuple(i.values()) for i in order_data], schema=list(order_data[0].keys()))\n",
    "spark_df_orders.show()\n",
    "\n",
    "BUCKET = \"soumil-dev-bucket-1995\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655ee953-01be-4041-bfb1-b8de3084d7e2",
   "metadata": {},
   "source": [
    "# UPSERT INTO HUDI TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94d3cee0-900d-4388-8a06-a801a562a2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "upsert_hudi_table(\n",
    "    glue_database=\"default\",\n",
    "    table_name=\"customers\",\n",
    "    record_id=\"customer_id\",\n",
    "    precomb_key=\"created_at\",\n",
    "    table_type='COPY_ON_WRITE',\n",
    "    partition_fields=\"\",\n",
    "    method='upsert',\n",
    "    index_type='BLOOM',\n",
    "    enable_partition=False,\n",
    "    enable_cleaner=False,\n",
    "    enable_hive_sync=True,\n",
    "    enable_clustering='False',\n",
    "    clustering_column='default',\n",
    "    enable_meta_data_indexing='false',\n",
    "    use_sql_transformer=False,\n",
    "    sql_transformer_query='default',\n",
    "    target_path=f\"s3://{BUCKET}/silver/table_name=customers/\",\n",
    "    spark_df=spark_df_customers,\n",
    ")\n",
    "\n",
    "upsert_hudi_table(\n",
    "    glue_database=\"default\",\n",
    "    table_name=\"orders\",\n",
    "    record_id=\"order_id\",\n",
    "    precomb_key=\"order_date\",\n",
    "    table_type='COPY_ON_WRITE',\n",
    "    partition_fields=\"default\",\n",
    "    method='upsert',\n",
    "    index_type='BLOOM',\n",
    "    enable_partition=False,\n",
    "    enable_cleaner=False,\n",
    "    enable_hive_sync=True,\n",
    "    enable_clustering='False',\n",
    "    clustering_column='default',\n",
    "    enable_meta_data_indexing='false',\n",
    "    use_sql_transformer=False,\n",
    "    sql_transformer_query='default',\n",
    "    target_path=f\"s3://{BUCKET}/silver/table_name=orders/\",\n",
    "    spark_df=spark_df_orders,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171000a5-d684-4679-8ed2-8313180ee76f",
   "metadata": {},
   "source": [
    "# Generate Some More order INcrementally processing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e9a594a-8a38-46e3-b1b8-e618d4fa01c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3c0980e-9fc7-4066-86ac-750e29ab44b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----------+--------+----------+--------------------+\n",
      "|            order_id|              name|order_value|priority|order_date|         customer_id|\n",
      "+--------------------+------------------+-----------+--------+----------+--------------------+\n",
      "|1f94e511-5073-40c...|      List writer.|        226|  MEDIUM|2023-12-09|3e908a54-2ea6-46c...|\n",
      "|6d938cb5-443d-4f6...|My PM PM study ok.|        259|     LOW|2023-11-30|3e908a54-2ea6-46c...|\n",
      "+--------------------+------------------+-----------+--------+----------+--------------------+"
     ]
    }
   ],
   "source": [
    "order_data = get_orders_data(\n",
    "    order_data_sample_size=order_data_sample_size,\n",
    "    customer_ids=[i.get(\"customer_id\") for i in customer_data]\n",
    ")\n",
    "spark_df_orders = spark.createDataFrame(data=[tuple(i.values()) for i in order_data], schema=list(order_data[0].keys()))\n",
    "spark_df_orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebc2fd31-49c7-47fe-81fe-22e33494966a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "upsert_hudi_table(\n",
    "    glue_database=\"default\",\n",
    "    table_name=\"orders\",\n",
    "    record_id=\"order_id\",\n",
    "    precomb_key=\"order_date\",\n",
    "    table_type='COPY_ON_WRITE',\n",
    "    partition_fields=\"default\",\n",
    "    method='upsert',\n",
    "    index_type='BLOOM',\n",
    "    enable_partition=False,\n",
    "    enable_cleaner=False,\n",
    "    enable_hive_sync=True,\n",
    "    enable_clustering='False',\n",
    "    clustering_column='default',\n",
    "    enable_meta_data_indexing='false',\n",
    "    use_sql_transformer=False,\n",
    "    sql_transformer_query='default',\n",
    "    target_path=f\"s3://{BUCKET}/silver/table_name=orders/\",\n",
    "    spark_df=spark_df_orders,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eaa22f38-1d78-4a47-8563-8ccda4d86f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13410507-2662-408b-a68e-fb76b0805407",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
